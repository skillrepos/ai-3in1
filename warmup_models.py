#!/usr/bin/env python3
"""
Model Warmup Script
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
Primes Ollama LLM and embedding models by loading them into memory.
This significantly reduces the first-run latency for labs 3, 5, 7, etc.

â±ï¸  WHY USE THIS SCRIPT?
   The first time you use an LLM or embedding model in a lab, there's a
   noticeable delay (30-60 seconds) as the model loads into memory. This
   script pre-loads the models so your labs start quickly.

ğŸ“‹ WHEN TO RUN:
   â€¢ After Lab 1, Step 4 (after `ollama pull llama3.2`)
   â€¢ Any time you restart Ollama (`ollama serve &`)
   â€¢ Before starting Labs 3, 5, 7, or any agent-based labs

âš¡ WHAT IT DOES:
   1. Loads llama3.2 LLM into Ollama's memory
   2. Downloads and loads the sentence transformer embedding model
   3. Verifies ChromaDB is available for vector storage
   4. Pre-populates MCP server's vector database (for Labs 6-7)

ğŸ’¡ TIP: Run this once at the start of your session, then all labs will be fast!

Usage:
    python warmup_models.py
"""

import os
import sys
import time
from pathlib import Path

print("=" * 60)
print("Model Warmup Script")
print("=" * 60)
print("\nThis script will load models into memory to reduce first-run latency.")
print("Please wait while models are warmed up...\n")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 1. Warm up Ollama LLM (llama3.2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("[1/3] Warming up Ollama LLM (llama3.2)...")
start = time.time()

try:
    from langchain_ollama import ChatOllama

    # Get model name from environment or use default
    model_name = os.getenv("OLLAMA_MODEL", "llama3.2")

    # Initialize the LLM (this loads the model into memory)
    llm = ChatOllama(model=model_name, temperature=0.0)

    # Make a simple test call to fully load the model
    print(f"   â€¢ Loading {model_name} into memory...")
    response = llm.invoke("Hello")

    elapsed = time.time() - start
    print(f"   âœ“ Ollama LLM loaded successfully ({elapsed:.1f}s)")
    print(f"   â€¢ Model: {model_name}")

except ImportError as e:
    print(f"   âœ— Error: langchain_ollama not installed")
    print(f"   â€¢ Install with: pip install langchain-ollama")
    sys.exit(1)

except Exception as e:
    print(f"   âœ— Error loading Ollama LLM: {e}")
    print(f"\n   Troubleshooting:")
    print(f"   â€¢ Is Ollama running? Try: ollama serve &")
    print(f"   â€¢ Is llama3.2 downloaded? Try: ollama pull llama3.2")
    print(f"   â€¢ Check Ollama status: ollama list")
    sys.exit(1)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 2. Warm up Sentence Transformer (all-MiniLM-L6-v2)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n[2/3] Warming up Sentence Transformer (all-MiniLM-L6-v2)...")
start = time.time()

try:
    from sentence_transformers import SentenceTransformer

    # Initialize the embedding model (this downloads and loads it)
    print("   â€¢ Loading embedding model into memory...")
    embed_model = SentenceTransformer("all-MiniLM-L6-v2")

    # Make a test encoding to fully load the model
    test_embedding = embed_model.encode("test")

    elapsed = time.time() - start
    print(f"   âœ“ Sentence Transformer loaded successfully ({elapsed:.1f}s)")
    print(f"   â€¢ Model: all-MiniLM-L6-v2")
    print(f"   â€¢ Embedding dimension: {len(test_embedding)}")

except ImportError:
    print(f"   âœ— Error: sentence-transformers not installed")
    print(f"   â€¢ Install with: pip install sentence-transformers")
    sys.exit(1)

except Exception as e:
    print(f"   âœ— Error loading Sentence Transformer: {e}")
    sys.exit(1)

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 3. Verify ChromaDB (optional - doesn't need warmup)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n[3/4] Verifying ChromaDB installation...")
start = time.time()

try:
    import chromadb
    from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE

    # Just verify import works - no need to create a client
    elapsed = time.time() - start
    print(f"   âœ“ ChromaDB available ({elapsed:.3f}s)")

    # Check if chroma_db directory exists
    chroma_path = Path("./chroma_db")
    if chroma_path.exists():
        print(f"   â€¢ Found existing ChromaDB at {chroma_path}")
    else:
        print(f"   â€¢ ChromaDB will be created when first used")

except ImportError:
    print(f"   âœ— Warning: chromadb not installed (needed for Labs 5, 7)")
    print(f"   â€¢ Install with: pip install chromadb")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# 4. Pre-populate MCP Server Vector Database (for Labs 6-7)
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n[4/4] Pre-populating MCP server vector database...")
start = time.time()

try:
    import pandas as pd
    import chromadb
    from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE
    from sentence_transformers import SentenceTransformer

    try:
        import pdfplumber
        pdf_available = True
    except ImportError:
        pdf_available = False
        print("   âš ï¸  pdfplumber not installed - skipping PDF data")

    # Paths
    MCP_CHROMA_PATH = Path("./mcp_chroma_db")
    OFFICE_CSV = Path("./data/offices.csv")
    OFFICE_PDF = Path("./data/offices.pdf")

    # Check if vector DB already populated
    if MCP_CHROMA_PATH.exists():
        client = chromadb.PersistentClient(
            path=str(MCP_CHROMA_PATH),
            settings=Settings(),
            tenant=DEFAULT_TENANT,
            database=DEFAULT_DATABASE,
        )

        try:
            locations = client.get_collection("office_locations")
            analytics = client.get_collection("office_analytics")

            if locations.count() > 0 and analytics.count() > 0:
                print(f"   âœ“ MCP vector DB already populated ({elapsed:.3f}s)")
                print(f"   â€¢ Locations: {locations.count()} documents")
                print(f"   â€¢ Analytics: {analytics.count()} documents")
                print(f"   â€¢ Path: {MCP_CHROMA_PATH}")
            else:
                raise ValueError("Collections exist but are empty")

        except Exception:
            # Collections don't exist or are empty, populate them
            print(f"   â€¢ Creating MCP vector database...")
            populate_needed = True
    else:
        print(f"   â€¢ Creating MCP vector database...")
        populate_needed = True

    # Populate if needed
    if 'populate_needed' in locals() and populate_needed:
        # Initialize
        client = chromadb.PersistentClient(
            path=str(MCP_CHROMA_PATH),
            settings=Settings(),
            tenant=DEFAULT_TENANT,
            database=DEFAULT_DATABASE,
        )
        locations_coll = client.get_or_create_collection("office_locations")
        analytics_coll = client.get_or_create_collection("office_analytics")

        # Use already-loaded embedding model
        print(f"   â€¢ Using pre-loaded embedding model...")

        # Populate locations from PDF
        if pdf_available and OFFICE_PDF.exists():
            print(f"   â€¢ Reading {OFFICE_PDF.name}...")
            with pdfplumber.open(OFFICE_PDF) as pdf:
                lines = []
                for page in pdf.pages:
                    text = page.extract_text() or ""
                    for line in text.split('\n'):
                        line = line.strip()
                        if line:
                            lines.append(line)

            print(f"   â€¢ Embedding {len(lines)} location documents...")
            for idx, line in enumerate(lines):
                vector = embed_model.encode(line).tolist()
                locations_coll.add(
                    ids=[f"pdf-{idx}"],
                    embeddings=[vector],
                    documents=[line],
                    metadatas=[{"source": "offices.pdf", "line": idx}],
                )
            print(f"   âœ“ Populated {len(lines)} location documents")
        else:
            if not pdf_available:
                print(f"   âš ï¸  Skipping PDF (pdfplumber not installed)")
            elif not OFFICE_PDF.exists():
                print(f"   âš ï¸  Skipping PDF ({OFFICE_PDF} not found)")

        # Populate analytics from CSV
        if OFFICE_CSV.exists():
            print(f"   â€¢ Reading {OFFICE_CSV.name}...")
            df = pd.read_csv(OFFICE_CSV)

            print(f"   â€¢ Embedding {len(df)} analytics documents...")
            for idx, row in df.iterrows():
                text = (f"{row['city']} office with {row['employees']} employees "
                       f"and ${row['revenue_million']}M revenue, opened in {row['opened_year']}")
                vector = embed_model.encode(text).tolist()
                analytics_coll.add(
                    ids=[f"csv-{idx}"],
                    embeddings=[vector],
                    documents=[text],
                    metadatas={
                        "source": "offices.csv",
                        "city": row['city'],
                        "employees": int(row['employees']),
                        "revenue_million": float(row['revenue_million']),
                        "opened_year": int(row['opened_year'])
                    },
                )
            print(f"   âœ“ Populated {len(df)} analytics documents")
        else:
            print(f"   âš ï¸  Skipping CSV ({OFFICE_CSV} not found)")

        elapsed = time.time() - start
        print(f"   âœ“ MCP vector DB populated successfully ({elapsed:.1f}s)")
        print(f"   â€¢ Path: {MCP_CHROMA_PATH}")

except ImportError as e:
    print(f"   âš ï¸  Skipping MCP vector DB (missing dependencies)")
    print(f"   â€¢ This is OK for Labs 3-5, needed for Labs 6-7")

except Exception as e:
    print(f"   âš ï¸  Could not populate MCP vector DB: {e}")
    print(f"   â€¢ MCP server will populate it on first startup")

# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# Summary
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
print("\n" + "=" * 60)
print("âœ“ Warmup Complete!")
print("=" * 60)
print("\nModels are now loaded in memory. Your labs should start faster!")
print("\nReady to run:")
print("  â€¢ Lab 3: python mcp_agent.py")
print("  â€¢ Lab 5: python rag_agent.py")
print("  â€¢ Lab 7: python rag_agent_classification.py")
print("\nNote: Models will remain in memory until Ollama is restarted.")
print("=" * 60)
