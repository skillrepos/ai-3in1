#!/usr/bin/env python3
"""
Model Warmup Script
‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
Primes Ollama LLM and embedding models by loading them into memory.
This significantly reduces the first-run latency for labs 3, 5, 7, etc.

‚è±Ô∏è  WHY USE THIS SCRIPT?
   The first time you use an LLM or embedding model in a lab, there's a
   noticeable delay (30-60 seconds) as the model loads into memory. This
   script pre-loads the models so your labs start quickly.

üìã WHEN TO RUN:
   ‚Ä¢ After Lab 1, Step 4 (after `ollama pull llama3.2`)
   ‚Ä¢ Any time you restart Ollama (`ollama serve &`)
   ‚Ä¢ Before starting Labs 3, 5, 7, or any agent-based labs

‚ö° WHAT IT DOES:
   1. Loads llama3.2 LLM into Ollama's memory
   2. Downloads and loads the sentence transformer embedding model
   3. Verifies ChromaDB is available for vector storage
   4. Pre-populates MCP server's vector database (for Labs 6-7)

üí° TIP: Run this once at the start of your session, then all labs will be fast!

Usage:
    python warmup_models.py
"""

import os
import sys
import time
from pathlib import Path

print("=" * 60)
print("Model Warmup Script")
print("=" * 60)
print("\nThis script will load models into memory to reduce first-run latency.")
print("Please wait while models are warmed up...\n")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 1. Warm up Ollama LLM (llama3.2)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
print("[1/3] Warming up Ollama LLM (llama3.2)...")
start = time.time()

try:
    from langchain_ollama import ChatOllama

    # Get model name from environment or use default
    model_name = os.getenv("OLLAMA_MODEL", "llama3.2")

    # Initialize the LLM (this loads the model into memory)
    llm = ChatOllama(model=model_name, temperature=0.0)

    # Make a simple test call to fully load the model
    print(f"   ‚Ä¢ Loading {model_name} into memory...")
    response = llm.invoke("Hello")

    elapsed = time.time() - start
    print(f"   ‚úì Ollama LLM loaded successfully ({elapsed:.1f}s)")
    print(f"   ‚Ä¢ Model: {model_name}")

except ImportError as e:
    print(f"   ‚úó Error: langchain_ollama not installed")
    print(f"   ‚Ä¢ Install with: pip install langchain-ollama")
    sys.exit(1)

except Exception as e:
    print(f"   ‚úó Error loading Ollama LLM: {e}")
    print(f"\n   Troubleshooting:")
    print(f"   ‚Ä¢ Is Ollama running? Try: ollama serve &")
    print(f"   ‚Ä¢ Is llama3.2 downloaded? Try: ollama pull llama3.2")
    print(f"   ‚Ä¢ Check Ollama status: ollama list")
    sys.exit(1)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 2. Warm up Sentence Transformer (all-MiniLM-L6-v2)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
print("\n[2/3] Warming up Sentence Transformer (all-MiniLM-L6-v2)...")
start = time.time()

try:
    from sentence_transformers import SentenceTransformer

    # Initialize the embedding model (this downloads and loads it)
    print("   ‚Ä¢ Loading embedding model into memory...")
    embed_model = SentenceTransformer("all-MiniLM-L6-v2")

    # Make a test encoding to fully load the model
    test_embedding = embed_model.encode("test")

    elapsed = time.time() - start
    print(f"   ‚úì Sentence Transformer loaded successfully ({elapsed:.1f}s)")
    print(f"   ‚Ä¢ Model: all-MiniLM-L6-v2")
    print(f"   ‚Ä¢ Embedding dimension: {len(test_embedding)}")

except ImportError:
    print(f"   ‚úó Error: sentence-transformers not installed")
    print(f"   ‚Ä¢ Install with: pip install sentence-transformers")
    sys.exit(1)

except Exception as e:
    print(f"   ‚úó Error loading Sentence Transformer: {e}")
    sys.exit(1)

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 3. Verify ChromaDB (optional - doesn't need warmup)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
print("\n[3/4] Verifying ChromaDB installation...")
start = time.time()

try:
    import chromadb
    from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE

    # Just verify import works - no need to create a client
    elapsed = time.time() - start
    print(f"   ‚úì ChromaDB available ({elapsed:.3f}s)")

    # Check if chroma_db directory exists
    chroma_path = Path("./chroma_db")
    if chroma_path.exists():
        print(f"   ‚Ä¢ Found existing ChromaDB at {chroma_path}")
    else:
        print(f"   ‚Ä¢ ChromaDB will be created when first used")

except ImportError:
    print(f"   ‚úó Warning: chromadb not installed (needed for Labs 5, 7)")
    print(f"   ‚Ä¢ Install with: pip install chromadb")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# 4. Pre-populate MCP Server Vector Database (for Labs 6-7)
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
print("\n[4/4] Pre-populating MCP server vector database...")
start = time.time()

try:
    import pandas as pd
    import chromadb
    from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE
    from sentence_transformers import SentenceTransformer

    try:
        import pdfplumber
        pdf_available = True
    except ImportError:
        pdf_available = False
        print("   ‚ö†Ô∏è  pdfplumber not installed - skipping PDF data")

    # Paths
    MCP_CHROMA_PATH = Path("./mcp_chroma_db")
    OFFICE_CSV = Path("./data/offices.csv")
    OFFICE_PDF = Path("./data/offices.pdf")

    # Check if vector DB already populated
    if MCP_CHROMA_PATH.exists():
        client = chromadb.PersistentClient(
            path=str(MCP_CHROMA_PATH),
            settings=Settings(),
            tenant=DEFAULT_TENANT,
            database=DEFAULT_DATABASE,
        )

        try:
            locations = client.get_collection("office_locations")
            analytics = client.get_collection("office_analytics")

            if locations.count() > 0 and analytics.count() > 0:
                print(f"   ‚úì MCP vector DB already populated ({elapsed:.3f}s)")
                print(f"   ‚Ä¢ Locations: {locations.count()} documents")
                print(f"   ‚Ä¢ Analytics: {analytics.count()} documents")
                print(f"   ‚Ä¢ Path: {MCP_CHROMA_PATH}")
            else:
                raise ValueError("Collections exist but are empty")

        except Exception:
            # Collections don't exist or are empty, populate them
            print(f"   ‚Ä¢ Creating MCP vector database...")
            populate_needed = True
    else:
        print(f"   ‚Ä¢ Creating MCP vector database...")
        populate_needed = True

    # Populate if needed
    if 'populate_needed' in locals() and populate_needed:
        # Initialize
        client = chromadb.PersistentClient(
            path=str(MCP_CHROMA_PATH),
            settings=Settings(),
            tenant=DEFAULT_TENANT,
            database=DEFAULT_DATABASE,
        )
        locations_coll = client.get_or_create_collection("office_locations")
        analytics_coll = client.get_or_create_collection("office_analytics")

        # Use already-loaded embedding model
        print(f"   ‚Ä¢ Using pre-loaded embedding model...")

        # Populate locations from PDF
        if pdf_available and OFFICE_PDF.exists():
            print(f"   ‚Ä¢ Reading {OFFICE_PDF.name}...")
            with pdfplumber.open(OFFICE_PDF) as pdf:
                lines = []
                for page in pdf.pages:
                    text = page.extract_text() or ""
                    for line in text.split('\n'):
                        line = line.strip()
                        if line:
                            lines.append(line)

            print(f"   ‚Ä¢ Embedding {len(lines)} location documents...")
            for idx, line in enumerate(lines):
                vector = embed_model.encode(line).tolist()
                locations_coll.add(
                    ids=[f"pdf-{idx}"],
                    embeddings=[vector],
                    documents=[line],
                    metadatas=[{"source": "offices.pdf", "line": idx}],
                )
            print(f"   ‚úì Populated {len(lines)} location documents")
        else:
            if not pdf_available:
                print(f"   ‚ö†Ô∏è  Skipping PDF (pdfplumber not installed)")
            elif not OFFICE_PDF.exists():
                print(f"   ‚ö†Ô∏è  Skipping PDF ({OFFICE_PDF} not found)")

        # Populate analytics from CSV
        if OFFICE_CSV.exists():
            print(f"   ‚Ä¢ Reading {OFFICE_CSV.name}...")
            df = pd.read_csv(OFFICE_CSV)

            print(f"   ‚Ä¢ Embedding {len(df)} analytics documents...")
            for idx, row in df.iterrows():
                text = (f"{row['city']} office with {row['employees']} employees "
                       f"and ${row['revenue_million']}M revenue, opened in {row['opened_year']}")
                vector = embed_model.encode(text).tolist()
                analytics_coll.add(
                    ids=[f"csv-{idx}"],
                    embeddings=[vector],
                    documents=[text],
                    metadatas={
                        "source": "offices.csv",
                        "city": row['city'],
                        "employees": int(row['employees']),
                        "revenue_million": float(row['revenue_million']),
                        "opened_year": int(row['opened_year'])
                    },
                )
            print(f"   ‚úì Populated {len(df)} analytics documents")
        else:
            print(f"   ‚ö†Ô∏è  Skipping CSV ({OFFICE_CSV} not found)")

        elapsed = time.time() - start
        print(f"   ‚úì MCP vector DB populated successfully ({elapsed:.1f}s)")
        print(f"   ‚Ä¢ Path: {MCP_CHROMA_PATH}")

except ImportError as e:
    print(f"   ‚ö†Ô∏è  Skipping MCP vector DB (missing dependencies)")
    print(f"   ‚Ä¢ This is OK for Labs 3-5, needed for Labs 6-7")

except Exception as e:
    print(f"   ‚ö†Ô∏è  Could not populate MCP vector DB: {e}")
    print(f"   ‚Ä¢ MCP server will populate it on first startup")

# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
# Summary
# ‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê
print("\n" + "=" * 60)
print("‚úì Warmup Complete!")
print("=" * 60)
print("\nModels are now loaded in memory. Your labs should start faster!")
print("\nReady to run:")
print("  ‚Ä¢ Lab 3: python mcp_agent.py")
print("  ‚Ä¢ Lab 5: python rag_agent.py")
print("  ‚Ä¢ Lab 7: python rag_agent_classification.py")
print("\nNote: Models will remain in memory until Ollama is restarted.")
print("=" * 60)
