================================================================================
LAB 5 ARCHITECTURE: RAG (Retrieval-Augmented Generation) with Agents
================================================================================

OVERVIEW
--------
Combines vector database search (Lab 4) with agentic behavior (Lab 2) to create
knowledge-grounded responses using RAG pipeline with memory integration.


SYSTEM ARCHITECTURE
-------------------

                    ┌──────────────────────────────────────────────────┐
                    │         RAG AGENT SYSTEM                         │
                    │                                                  │
                    │  ┌────────────────────────────────────────────┐  │
                    │  │  AGENT WITH RAG CAPABILITIES               │  │
                    │  │                                            │  │
                    │  │  ┌──────────────────────────────────────┐  │  │
                    │  │  │ TAO LOOP (Enhanced)                  │  │  │
                    │  │  │                                      │  │  │
                    │  │  │  1. THOUGHT                          │  │  │
                    │  │  │     - Analyze query                  │  │  │
                    │  │  │     - Decide: RAG or Tool?           │  │  │
                    │  │  │                                      │  │  │
                    │  │  │  2. ACTION                           │  │  │
                    │  │  │     - Vector search OR               │  │  │
                    │  │  │     - Tool call                      │  │  │
                    │  │  │                                      │  │  │
                    │  │  │  3. OBSERVATION                      │  │  │
                    │  │  │     - Process retrieved docs         │  │  │
                    │  │  │     - Store in memory                │  │  │
                    │  │  └──────────────────────────────────────┘  │  │
                    │  └────────────────────────────────────────────┘  │
                    │                    │                             │
                    │                    ▼                             │
                    │  ┌────────────────────────────────────────────┐  │
                    │  │  RETRIEVAL LAYER                           │  │
                    │  │                                            │  │
                    │  │  ┌──────────────┐     ┌──────────────┐    │  │
                    │  │  │ Vector Memory│     │ ChromaDB     │    │  │
                    │  │  │ (Semantic)   │     │ (Documents)  │    │  │
                    │  │  │              │     │              │    │  │
                    │  │  │ Past queries │     │ offices.pdf  │    │  │
                    │  │  │ & answers    │     │ *.py files   │    │  │
                    │  │  └──────────────┘     └──────────────┘    │  │
                    │  └────────────────────────────────────────────┘  │
                    │                    │                             │
                    │                    ▼                             │
                    │  ┌────────────────────────────────────────────┐  │
                    │  │  GENERATION LAYER                          │  │
                    │  │                                            │  │
                    │  │  Input: Query + Retrieved Context          │  │
                    │  │  Output: Grounded Response                 │  │
                    │  │                                            │  │
                    │  │  Prompt Template:                          │  │
                    │  │  ┌──────────────────────────────────────┐  │  │
                    │  │  │ Context: {retrieved_docs}            │  │  │
                    │  │  │ Query: {user_query}                  │  │  │
                    │  │  │ Answer based on context only.        │  │  │
                    │  │  └──────────────────────────────────────┘  │  │
                    │  └────────────────────────────────────────────┘  │
                    └──────────────────────────────────────────────────┘


RAG PIPELINE FLOW
-----------------

Query: "Which office has the highest revenue?"

    ┌─────────────────────┐
    │ 1. USER QUERY       │
    │    "highest revenue"│
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ 2. EMBED QUERY      │
    │    all-MiniLM-L6-v2 │
    │    → [0.16,-0.44,...]│
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ 3. VECTOR SEARCH    │
    │    ChromaDB         │
    │    Top-K=5          │
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ 4. RETRIEVE DOCS    │
    │    1. NY: $85.5M    │
    │    2. SF: $78.2M    │
    │    3. CHI: $52.3M   │
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ 5. BUILD PROMPT     │
    │    Context: [docs]  │
    │    Query: [Q]       │
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ 6. LLM GENERATE     │
    │    Process context  │
    │    Generate answer  │
    └──────────┬──────────┘
               │
               ▼
    ┌─────────────────────┐
    │ 7. GROUNDED ANSWER  │
    │    "New York office │
    │    has highest      │
    │    revenue: $85.5M" │
    └─────────────────────┘


VECTOR MEMORY SYSTEM (Lab 5)
----------------------------

┌──────────────────────────────────────────────────────────────┐
│ LAB 5 MEMORY ARCHITECTURE                                    │
├──────────────────────────────────────────────────────────────┤
│                                                              │
│  ┌──────────────────────────────────────────────────────┐   │
│  │ VECTOR MEMORY (Semantic Search)                      │   │
│  │                                                      │   │
│  │ Purpose:                                             │   │
│  │ • Long-term conversation recall                     │   │
│  │ • Semantic similarity matching                       │   │
│  │ • Topic clustering across sessions                   │   │
│  │                                                      │   │
│  │ Storage:                                             │   │
│  │ • ChromaDB collection: "conversation_memory"         │   │
│  │ • Persistent disk storage                            │   │
│  │ • Survives restarts                                  │   │
│  │                                                      │   │
│  │ Retrieval:                                           │   │
│  │ • Cosine similarity on embeddings                    │   │
│  │ • Top-K (K=3 by default)                             │   │
│  │ • Query embedding vs stored query embeddings         │   │
│  │                                                      │   │
│  │ Functions:                                           │   │
│  │ • store_conversation(query, response, model)         │   │
│  │ • retrieve_relevant_conversations(query, model)      │   │
│  │ • get_memory_stats()                                 │   │
│  │                                                      │   │
│  │ Use Case:                                            │   │
│  │ "What did we discuss about revenue?"                 │   │
│  │ → Finds similar past conversations semantically      │   │
│  └──────────────────────────────────────────────────────┘   │
│                             │                                │
│                             ▼                                │
│              ┌──────────────────────────┐                    │
│              │ COMBINED CONTEXT         │                    │
│              │                          │                    │
│              │ Vector Memory +          │                    │
│              │ Retrieved Docs           │                    │
│              └──────────────────────────┘                    │
│                                                              │
│  Note: Lab 2.5 uses buffer memory (deque), Lab 5 uses       │
│        vector memory (ChromaDB). Different approaches!       │
└──────────────────────────────────────────────────────────────┘


QUERY ROUTING LOGIC
-------------------

User Query
    │
    ▼
┌─────────────────────┐
│ Query Analysis      │
│                     │
│ Is this about:      │
│ • Documents?        │
│ • Past conversation?│
│ • Real-time data?   │
└──────────┬──────────┘
           │
           ├──────────────────┬──────────────────┐
           │                  │                  │
           ▼                  ▼                  ▼
    ┌──────────┐      ┌──────────┐      ┌──────────┐
    │ RAG Path │      │ Memory   │      │ Tool     │
    │          │      │ Path     │      │ Path     │
    └────┬─────┘      └────┬─────┘      └────┬─────┘
         │                 │                  │
         ▼                 ▼                  ▼
    ┌──────────┐      ┌──────────┐      ┌──────────┐
    │ Vector   │      │ Vector   │      │ Call     │
    │ Search   │      │ Memory   │      │ get_     │
    │ ChromaDB │      │ Search   │      │ weather  │
    └────┬─────┘      └────┬─────┘      └────┬─────┘
         │                 │                  │
         └────────┬────────┴──────────────────┘
                  │
                  ▼
           ┌──────────────┐
           │ Generate     │
           │ Response     │
           └──────────────┘


EXAMPLE QUERY FLOW WITH MEMORY
-------------------------------

Query 1: "What office has highest revenue?"

Step 1: Check Buffer Memory → Empty
Step 2: Search ChromaDB → Find "NY: $85.5M"
Step 3: Generate: "New York office has $85.5M"
Step 4: Store in BOTH memories:
        - Buffer: Q&A exchange
        - Vector: Embedded Q&A for semantic recall

Query 2: "What about the second highest?"

Step 1: Check Buffer Memory → Has "NY: $85.5M" context
Step 2: Search ChromaDB → Find "SF: $78.2M"
Step 3: Generate: "San Francisco is second at $78.2M"
Step 4: Store in both memories

Query 3: "What did we discuss about revenue?" (1 hour later)

Step 1: Check Buffer Memory → May be cleared
Step 2: Search Vector Memory → Find past Q&A about revenue
Step 3: Generate: "We discussed NY ($85.5M) and SF ($78.2M)"


PROMPT TEMPLATE STRUCTURE
--------------------------

┌────────────────────────────────────────────────────────────┐
│ SYSTEM PROMPT                                              │
├────────────────────────────────────────────────────────────┤
│ You are a helpful assistant with access to documents.     │
│ Always answer based on the provided context.              │
│ If context doesn't contain the answer, say so.            │
└────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────────┐
│ CONTEXT FROM RETRIEVAL                                     │
├────────────────────────────────────────────────────────────┤
│ Document 1: New York Office                                │
│ 123 Main St, New York, NY                                  │
│ 120 employees, $85.5M revenue                              │
│ Opened: 2005                                               │
│                                                            │
│ Document 2: San Francisco Office                          │
│ 456 Tech Blvd, San Francisco, CA                           │
│ 95 employees, $78.2M revenue                               │
│ Opened: 2010                                               │
└────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────────┐
│ MEMORY CONTEXT (if available)                              │
├────────────────────────────────────────────────────────────┤
│ Previous conversation:                                     │
│ User: "Which office has most employees?"                   │
│ Agent: "New York with 120 employees"                       │
└────────────────────────────────────────────────────────────┘

┌────────────────────────────────────────────────────────────┐
│ USER QUERY                                                 │
├────────────────────────────────────────────────────────────┤
│ What about revenue?                                        │
└────────────────────────────────────────────────────────────┘


INFORMATION EXTRACTION
----------------------

Raw Document:
┌────────────────────────────────────┐
│ New York Office                    │
│ 123 Main St, New York, NY 10001    │
│ Phone: (212) 555-0100              │
│ 120 employees                      │
│ Annual revenue: $85.5M             │
│ Departments: Sales, Marketing...   │
│ Manager: John Smith                │
│ Opened: January 2005               │
└────────────────────────────────────┘

After Chunking (512 chars):
┌────────────────────────────────────┐
│ Chunk 1:                           │
│ "New York Office 123 Main St,      │
│  New York, NY 10001 Phone: (212)   │
│  555-0100 120 employees Annual     │
│  revenue: $85.5M Departments:      │
│  Sales, Marketing, Engineering,    │
│  Finance Manager: John Smith       │
│  Opened: January 2005..."          │
└────────────────────────────────────┘

After Embedding:
Vector: [0.145, -0.423, 0.887, ..., 0.234] (384-dim)

Stored in ChromaDB:
{
  "id": "offices_chunk_001",
  "document": "New York Office...",
  "embedding": [0.145, -0.423, ...],
  "metadata": {"source": "offices.pdf", "page": 1}
}


RETRIEVAL STRATEGIES
--------------------

Strategy 1: SIMPLE RETRIEVAL
┌────────────────────────────────────┐
│ 1. Embed query                     │
│ 2. Search vector DB                │
│ 3. Return top-K                    │
│ 4. Use all K docs in context      │
└────────────────────────────────────┘

Strategy 2: RERANKING (Advanced)
┌────────────────────────────────────┐
│ 1. Embed query                     │
│ 2. Search vector DB (top-20)       │
│ 3. Rerank by relevance (top-5)     │
│ 4. Use reranked docs in context   │
└────────────────────────────────────┘

Strategy 3: HYBRID (Keyword + Vector)
┌────────────────────────────────────┐
│ 1. Keyword search → Results A      │
│ 2. Vector search → Results B       │
│ 3. Merge A + B                     │
│ 4. Deduplicate                     │
│ 5. Use merged docs in context     │
└────────────────────────────────────┘


COMPONENTS
----------

1. RAG AGENT (rag_agent.py)
   - Manages RAG pipeline
   - Routes queries (RAG vs Tool)
   - Integrates vector memory (built-in)
   - Generates grounded responses

   Memory Functions:
   - open_memory_collection() - Creates/retrieves memory collection
   - store_conversation(query, response, model) - Stores conversations
   - retrieve_relevant_conversations(query, model) - Semantic search
   - get_memory_stats() - Returns memory statistics

2. VECTOR MEMORY (integrated in rag_agent.py)
   - Stores conversation embeddings in ChromaDB
   - Semantic search over past exchanges
   - ChromaDB collection: "conversation_memory"
   - Persistent storage on disk
   - Top-K retrieval (K=3 by default)

3. DOCUMENT STORE (ChromaDB)
   - Collection: "codebase"
   - Indexed documents (offices.pdf, *.py)
   - Vector + text storage
   - HNSW index for search
   - Separate from memory collection


PERFORMANCE CHARACTERISTICS
----------------------------

Operation              Without RAG    With RAG
─────────              ───────────    ────────
Query latency          2-4s           3-6s
Context size           500 tokens     1500-3000 tokens
Accuracy               Low (guessing) High (grounded)
Hallucination rate     High           Low
Token cost/query       Base           +500-1000 tokens

Retrieval Performance:
  - Vector search: 10-30 ms (1K docs)
  - Embedding query: 20-50 ms
  - Document chunking: Offline (indexing phase)
  - Total retrieval: 30-80 ms

Memory Performance:
  - Vector memory search: 10-30 ms
  - Memory overhead: 10-30 ms per query
  - Persistent storage (no warmup needed)


COMPARISON: AGENT VS RAG AGENT
-------------------------------

LAB 2 AGENT (No RAG)
┌──────────────────────────┐
│ Query: "What's NY        │
│         revenue?"        │
│                          │
│ Response: "I don't have  │
│ access to that info"     │
│                          │
│ ✗ No knowledge           │
│ ✗ Can't answer           │
└──────────────────────────┘

LAB 5 RAG AGENT
┌──────────────────────────┐
│ Query: "What's NY        │
│         revenue?"        │
│                          │
│ 1. Search docs ──────┐   │
│                      │   │
│ 2. Find: "NY $85.5M" │   │
│                      │   │
│ 3. Response: "New York   │
│    office has $85.5M     │
│    revenue"              │
│                          │
│ ✓ Grounded in docs       │
│ ✓ Accurate answer        │
└──────────────────────────┘


KEY LEARNING POINTS
-------------------

1. RAG PIPELINE
   - Retrieval: Find relevant documents
   - Augmentation: Add docs to prompt
   - Generation: LLM uses context

2. GROUNDING
   - Responses based on retrieved docs
   - Reduces hallucination
   - Provides source attribution

3. DUAL MEMORY
   - Buffer: Recent conversation flow
   - Vector: Semantic long-term recall
   - Complementary strengths

4. CONTEXT WINDOW MANAGEMENT
   - Limited token budget
   - Select most relevant docs
   - Balance: memory + retrieval + query

5. PROMPT ENGINEERING
   - System prompt sets behavior
   - Context injection format
   - Instruction to cite sources


LIMITATIONS
-----------

- Retrieval quality depends on indexing
- Context window limits (can't fit all docs)
- Embedding model quality matters
- No reasoning about missing info
- Chunk boundaries may split important info
- Token costs increase with context


USE CASES
---------

✓ Question answering over documents
✓ Document-grounded chatbots
✓ Customer support with knowledge base
✓ Research assistants
✓ Code documentation Q&A
✓ Legal document analysis


NEXT STEPS
----------

Lab 6: Add classification to route queries intelligently
Lab 7: Combine RAG + Classification for smart routing
Lab 8: Web UI for user-friendly interaction

================================================================================
