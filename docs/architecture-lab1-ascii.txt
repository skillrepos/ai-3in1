================================================================================
LAB 1 ARCHITECTURE: Using Ollama to Run Models Locally
================================================================================

OVERVIEW
--------
Local LLM execution using Ollama, demonstrating both interactive CLI and
API-based interaction patterns.


SYSTEM ARCHITECTURE
-------------------

                    ┌─────────────────────────────────────┐
                    │      LOCAL MACHINE                  │
                    │                                     │
                    │  ┌───────────────────────────────┐  │
                    │  │   OLLAMA SERVICE              │  │
                    │  │   Port: 11434                 │  │
                    │  │                               │  │
                    │  │   ┌───────────────────────┐   │  │
                    │  │   │  Ollama Server        │   │  │
                    │  │   │  (Inference Engine)   │   │  │
                    │  │   └───────────────────────┘   │  │
                    │  │            │                  │  │
                    │  │            ▼                  │  │
                    │  │   ┌───────────────────────┐   │  │
                    │  │   │  Model Storage        │   │  │
                    │  │   │  ~/.ollama/models     │   │  │
                    │  │   │                       │   │  │
                    │  │   │  • llama3.2 (3B)      │   │  │
                    │  │   │  • GGUF format        │   │  │
                    │  │   └───────────────────────┘   │  │
                    │  └───────────────────────────────┘  │
                    │            ▲         ▲              │
                    │            │         │              │
                    │   ┌────────┘         └────────┐     │
                    │   │                           │     │
                    │   │                           │     │
                    │  ┌▼────────────┐    ┌────────▼───┐ │
                    │  │ CLI         │    │ HTTP API   │ │
                    │  │ Interface   │    │ Client     │ │
                    │  │             │    │            │ │
                    │  │ ollama run  │    │ curl/POST  │ │
                    │  └─────────────┘    └────────────┘ │
                    └─────────────────────────────────────┘
                              ▲
                              │
                    ┌─────────┴──────────┐
                    │   INTERNET         │
                    │   ollama.com       │
                    │   (Model Download) │
                    └────────────────────┘


DATA FLOW: MODEL DOWNLOAD
--------------------------

    User Command              Internet             Local Storage
    ─────────────             ────────             ─────────────

    $ ollama pull ──────────▶ ollama.com ──────────▶ ~/.ollama/
      llama3.2                (Download)            models/
                              ~1.5GB                llama3.2/


DATA FLOW: CLI INTERACTION
---------------------------

    User                CLI                Ollama Server        Model
    ────                ───                ─────────────        ─────

    Type query ─────▶ Parse ───────────▶ Load model ──────▶ Inference
                                                                  │
    Display    ◀──── Format ◀───────── Generate text ◀──────────┘
    response          output            response


DATA FLOW: API INTERACTION
---------------------------

    HTTP Client          Ollama API Server        Model
    ───────────          ─────────────────        ─────

    POST /api/      ──▶ Parse JSON      ────────▶ Inference
    generate             request                      │
    {                                                  │
      "model":                                         │
      "prompt":                                        │
    }                                                  │
                                                       │
    Response        ◀── JSON format    ◀──────────────┘
    {                   response
      "response":
      "done": true
    }


COMPONENTS
----------

1. OLLAMA SERVER
   - Purpose: LLM inference engine
   - Port: 11434
   - Functions:
     * Model management (pull, list, run, remove)
     * Inference execution
     * API endpoint serving
     * Model caching

2. MODEL STORAGE
   - Location: ~/.ollama/models/
   - Format: GGUF (GPT-Generated Unified Format)
   - Models: llama3.2 (3B parameters, ~1.5GB)
   - Optimizations: Quantization for CPU efficiency

3. CLI INTERFACE
   - Command: ollama run llama3.2
   - Mode: Interactive chat
   - Features: Stream responses, multi-turn conversation
   - Exit: /bye or Ctrl+D

4. HTTP API
   - Endpoint: http://localhost:11434/api/generate
   - Method: POST
   - Format: JSON
   - Modes: Streaming or single response


INTERACTION PATTERNS
---------------------

Pattern 1: INTERACTIVE CLI
┌──────────────────────────────────────────┐
│ $ ollama run llama3.2                    │
│ >>> Send a message (/? for help)         │
│                                          │
│ User: What's the weather in Paris?       │
│                                          │
│ Assistant: I don't have access to       │
│ real-time weather data...               │
│                                          │
│ User: /bye                               │
└──────────────────────────────────────────┘

Pattern 2: API CALL
┌──────────────────────────────────────────┐
│ $ curl http://localhost:11434/api/       │
│   generate -d '{                         │
│     "model": "llama3.2",                 │
│     "prompt": "What causes weather?",    │
│     "stream": false                      │
│   }'                                     │
│                                          │
│ Response:                                │
│ {                                        │
│   "model": "llama3.2",                   │
│   "response": "Weather is caused by...", │
│   "done": true                           │
│ }                                        │
└──────────────────────────────────────────┘


SIMPLE PYTHON EXAMPLE
---------------------

File: simple_ollama.py

┌────────────────────────────────────────────────────────────┐
│ # simple_ollama.py - Basic Ollama interaction              │
│                                                            │
│ from langchain_ollama import ChatOllama                    │
│                                                            │
│ # Initialize Ollama client                                 │
│ llm = ChatOllama(model="llama3.2")                         │
│                                                            │
│ # Get user input                                           │
│ user_prompt = input("Enter your prompt: ")                 │
│                                                            │
│ # Send to Ollama and get response                          │
│ response = llm.invoke(user_prompt)                         │
│                                                            │
│ # Display result                                           │
│ print(f"\nResponse: {response.content}")                   │
└────────────────────────────────────────────────────────────┘

Usage Example:
┌────────────────────────────────────────────────────────────┐
│ $ python simple_ollama.py                                  │
│ Enter your prompt: What is the capital of France?          │
│                                                            │
│ Response: The capital of France is Paris.                  │
└────────────────────────────────────────────────────────────┘

Execution Flow:
    User Input
        │
        ▼
    ┌─────────────────┐
    │ Python Script   │
    │ simple_ollama.py│
    └────────┬────────┘
             │
             ▼
    ┌─────────────────┐
    │ ChatOllama      │
    │ (LangChain)     │
    └────────┬────────┘
             │ HTTP
             ▼
    ┌─────────────────┐
    │ Ollama Server   │
    │ :11434          │
    └────────┬────────┘
             │
             ▼
    ┌─────────────────┐
    │ llama3.2 Model  │
    │ Inference       │
    └────────┬────────┘
             │
             ▼
         Response


PERFORMANCE CHARACTERISTICS
----------------------------

Metric                  Value
------                  -----
Model Size              1.5 GB (3B parameters)
Download Time           2-5 minutes (broadband)
Load Time               2-3 seconds
Inference Speed         ~10-20 tokens/sec (CPU)
                        ~50-100 tokens/sec (GPU)
Memory Usage            ~4 GB RAM minimum
Concurrent Users        1 (local instance)


KEY LEARNING POINTS
-------------------

1. LOCAL EXECUTION
   - No cloud dependency
   - Data privacy (runs entirely local)
   - No API costs

2. TWO INTERFACES
   - CLI: Interactive, human-friendly
   - API: Programmatic, automation-ready

3. MODEL MANAGEMENT
   - Pull models on demand
   - Multiple models can coexist
   - Easy switching between models

4. INFERENCE BASICS
   - Understanding latency
   - Token-based generation
   - Streaming vs batch responses


LIMITATIONS
-----------

- Single user at a time
- Resource intensive (CPU/RAM)
- No built-in multi-tenancy
- Limited to models that fit in memory
- CPU inference slower than GPU


NEXT STEPS
----------

Lab 2: Add tool-calling capabilities to create an agent
       that can perform actions (weather lookup)

================================================================================
