#!/usr/bin/env python3
"""
────────────────────────────────────────────────────────────────────────
Agent with Conversation Memory - Lab 2.5 (Optional) - COMPLETE SOLUTION
────────────────────────────────────────────────────────────────────────

This extends the basic agent from Lab 2 with memory capabilities.

**What is Memory?**
Without memory, each query is independent - the agent has no context of
previous questions. With memory, the agent can:
  - Remember what you asked before
  - Provide context-aware responses
  - Handle follow-up questions naturally

**Memory Types Demonstrated:**
1. Buffer Memory - Keeps last N conversations in a list
2. Token Limit Handling - Shows what happens when memory fills up
3. Memory Visualization - See memory status in real-time

Run the script and try multi-turn conversations to see memory in action!
"""

# ───────────────────────── standard library ─────────────────────────
import json
import textwrap
from typing import Dict, List
from collections import deque
from datetime import datetime

import requests                           # simple HTTP client

# ───────────────────────── 3rd-party libraries ──────────────────────
from langchain_ollama import ChatOllama

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 1.  Weather code lookup (same as Lab 2)                          ║
# ╚══════════════════════════════════════════════════════════════════╝
WEATHER_CODES: Dict[int, str] = {
    0:  "Clear sky",                     1:  "Mainly clear",
    2:  "Partly cloudy",                 3:  "Overcast",
    45: "Fog",                           48: "Depositing rime fog",
    51: "Light drizzle",                 53: "Moderate drizzle",
    55: "Dense drizzle",                 56: "Light freezing drizzle",
    57: "Dense freezing drizzle",        61: "Slight rain",
    63: "Moderate rain",                 65: "Heavy rain",
    66: "Light freezing rain",           67: "Heavy freezing rain",
    71: "Slight snow fall",              73: "Moderate snow fall",
    75: "Heavy snow fall",               77: "Snow grains",
    80: "Slight rain showers",           81: "Moderate rain showers",
    82: "Violent rain showers",          85: "Slight snow showers",
    86: "Heavy snow showers",            95: "Thunderstorm",
    96: "Thunderstorm with slight hail", 99: "Thunderstorm with heavy hail",
}

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 2.  NEW: Conversation Memory Manager                             ║
# ╚══════════════════════════════════════════════════════════════════╝
class ConversationMemory:
    """
    Manages conversation history with a fixed-size buffer.

    This is a simple "sliding window" memory that keeps the most recent
    exchanges. When the buffer is full, old conversations are dropped.
    """

    def __init__(self, max_exchanges: int = 5):
        """
        Initialize memory buffer.

        Args:
            max_exchanges: Maximum number of Q&A pairs to remember
        """
        self.memory = deque(maxlen=max_exchanges)
        self.max_exchanges = max_exchanges

    def add_exchange(self, user_query: str, agent_response: str):
        """
        Add a user-agent exchange to memory.

        Args:
            user_query: What the user asked
            agent_response: What the agent responded
        """
        exchange = {
            "user": user_query,
            "agent": agent_response,
            "timestamp": datetime.now().strftime("%H:%M:%S")
        }
        self.memory.append(exchange)

    def get_context_string(self) -> str:
        """
        Format memory as a context string for the LLM.

        Returns:
            Formatted string of previous conversations
        """
        if not self.memory:
            return ""

        context_parts = ["Previous conversation context:"]
        for exchange in self.memory:
            context_parts.append(f"User: {exchange['user']}")
            context_parts.append(f"Agent: {exchange['agent']}")

        return "\n".join(context_parts)

    def get_summary(self) -> str:
        """Get a summary of current memory status."""
        return f"{len(self.memory)}/{self.max_exchanges} exchanges stored"

    def clear(self):
        """Clear all memory."""
        self.memory.clear()

    def is_empty(self) -> bool:
        """Check if memory is empty."""
        return len(self.memory) == 0

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 3.  Tool functions (same as Lab 2)                               ║
# ╚══════════════════════════════════════════════════════════════════╝
def get_weather(lat: float, lon: float) -> dict:
    """Get weather forecast from Open-Meteo API."""
    url = "https://api.open-meteo.com/v1/forecast"
    params = {
        "latitude": lat,
        "longitude": lon,
        "daily": "temperature_2m_max,temperature_2m_min,weathercode",
        "timezone": "auto",
        "forecast_days": 1,
    }
    resp = requests.get(url, params=params, timeout=10)
    resp.raise_for_status()
    data = resp.json()
    daily = data["daily"]

    return {
        "high_c": daily["temperature_2m_max"][0],
        "low_c": daily["temperature_2m_min"][0],
        "code": daily["weathercode"][0],
        "conditions": WEATHER_CODES.get(daily["weathercode"][0], "Unknown"),
    }

def convert_c_to_f(c: float) -> float:
    """Convert Celsius to Fahrenheit."""
    return c * 9 / 5 + 32

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 4.  LLM wrapper (same as Lab 2)                                  ║
# ╚══════════════════════════════════════════════════════════════════╝
llm = ChatOllama(model="llama3.2", base_url="http://localhost:11434")

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 5.  System prompt (same as Lab 2)                                ║
# ╚══════════════════════════════════════════════════════════════════╝
SYSTEM_PROMPT = textwrap.dedent("""
You are a helpful weather assistant. You have access to two tools:

1. get_weather(lat, lon) - Returns weather forecast
   Args: {"lat": <latitude>, "lon": <longitude>}

2. convert_c_to_f(c) - Converts Celsius to Fahrenheit
   Args: {"c": <temperature_celsius>}

Use the TAO (Thought-Action-Observation) format:
- Thought: Your reasoning
- Action: get_weather or convert_c_to_f
- Args: JSON arguments

After observations, provide a final friendly answer.
""").strip()

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 6.  MODIFIED: Agent runner with memory support                   ║
# ╚══════════════════════════════════════════════════════════════════╝
def run_with_memory(question: str, memory: ConversationMemory) -> str:
    """
    Execute TAO loop with conversation memory.

    Args:
        question: User's question
        memory: Conversation memory object

    Returns:
        Agent's response
    """
    # Build conversation history including memory context
    messages = [{"role": "system", "content": SYSTEM_PROMPT}]

    # Add memory context if available
    if not memory.is_empty():
        memory_context = memory.get_context_string()
        messages.append({"role": "system", "content": memory_context})
        print(f"[Using memory: {memory.get_summary()}]")

    messages.append({"role": "user", "content": question})

    # First planning step: get coordinates
    plan1 = llm.invoke(messages).content
    print(f"Thought/Action 1:\n{plan1}\n")

    coords = json.loads(plan1.split("Args:")[1].strip())
    obs1 = get_weather(**coords)
    print(f"Observation 1: {obs1}\n")

    # Second planning step: convert units
    messages.append({"role": "assistant", "content": plan1})
    messages.append({"role": "user", "content": f"Observation: {obs1}"})

    plan2 = llm.invoke(messages).content
    print(f"Thought/Action 2:\n{plan2}\n")

    high_f = convert_c_to_f(obs1["high_c"])
    low_f = convert_c_to_f(obs1["low_c"])

    final = (
        f"Today will be **{obs1['conditions']}** with a high of "
        f"**{high_f:.1f} °F** and a low of **{low_f:.1f} °F**."
    )
    print(f"Final: {final}\n")
    return final

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 7.  Interactive REPL with memory visualization                   ║
# ╚══════════════════════════════════════════════════════════════════╝
def print_memory_status(memory: ConversationMemory):
    """Display current memory status."""
    print(f"\n{'='*60}")
    print(f"Memory Status: {memory.get_summary()}")
    if not memory.is_empty():
        print(f"Context available for next query")
    print(f"{'='*60}\n")

if __name__ == "__main__":
    print("Weather Agent with Memory (type 'exit' to quit, 'clear' to reset memory)\n")
    print("TIP: Try asking about multiple cities, then refer back to previous ones!")
    print("     Example: 'What about Paris?', then 'How does that compare to the last city?'\n")

    # Initialize conversation memory
    memory = ConversationMemory(max_exchanges=5)

    while True:
        print_memory_status(memory)

        loc = input("Location (or 'exit'/'clear'): ").strip()

        if loc.lower() == "exit":
            print("Goodbye!")
            break

        if loc.lower() == "clear":
            memory.clear()
            print("Memory cleared!\n")
            continue

        # Build query
        query = (
            f"What is the predicted weather today for {loc}? "
            "Include conditions plus high/low in °F."
        )

        try:
            response = run_with_memory(query, memory)
            memory.add_exchange(query, response)
        except Exception as e:
            print(f"Warning: Error: {e}\n")
