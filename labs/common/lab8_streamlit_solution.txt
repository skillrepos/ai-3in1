#!/usr/bin/env python3
"""
Streamlit Web Application for Classification-Based RAG Agent
============================================================

This web app provides a user-friendly interface for our classification query system.
Features:
- Chat-like interface for natural language queries
- Real-time display of classification process
- Confidence scores and alternative query suggestions
- Support for both weather and office data queries
- Visual indicators for processing steps
"""

import asyncio
import streamlit as st
import time
import json
from pathlib import Path
import sys
from datetime import datetime

# Add current directory to path
sys.path.insert(0, str(Path(__file__).parent))

# Import our agent
from rag_agent_classification import process_query

# Page configuration
st.set_page_config(
    page_title="AI Office Assistant",
    page_icon="ðŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS for better appearance
st.markdown("""
<style>
    .main-header {
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .query-box {
        background-color: #f0f2f6;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
    }
    .result-box {
        background-color: #e8f4fd;
        padding: 1rem;
        border-radius: 0.5rem;
        margin: 1rem 0;
        border-left: 4px solid #1f77b4;
    }
    .confidence-high { color: #28a745; font-weight: bold; }
    .confidence-medium { color: #ffc107; font-weight: bold; }
    .confidence-low { color: #dc3545; font-weight: bold; }
    .processing-step {
        background-color: #fff3cd;
        padding: 0.5rem;
        border-radius: 0.25rem;
        margin: 0.25rem 0;
        border-left: 3px solid #ffc107;
    }
</style>
""", unsafe_allow_html=True)

def get_confidence_class(confidence):
    """Get CSS class based on confidence score."""
    if confidence >= 0.8:
        return "confidence-high"
    elif confidence >= 0.5:
        return "confidence-medium"
    else:
        return "confidence-low"

def get_confidence_indicator(confidence):
    """Get indicator based on confidence score."""
    if confidence >= 0.8:
        return "[HIGH]"
    elif confidence >= 0.5:
        return "[MEDIUM]"
    else:
        return "[LOW]"

async def process_query_with_progress(user_query):
    """Process query and show progress steps."""
    
    # Create progress containers
    progress_container = st.container()
    result_container = st.container()
    
    with progress_container:
        st.markdown("### Processing Steps")
        
        # Step indicators
        step1 = st.empty()
        step2 = st.empty()
        step3 = st.empty()
        step4 = st.empty()
        
        # Step 1: Query Analysis
        with step1:
            st.markdown('<div class="processing-step">[1/4] Analyzing query intent...</div>', unsafe_allow_html=True)
        
        time.sleep(0.5)  # Simulate processing time
        
        # Step 2: Route Detection
        user_lower = user_query.lower()
        weather_keywords = ["weather", "temperature", "forecast", "conditions", "climate"]
        is_weather = any(keyword in user_lower for keyword in weather_keywords)
        
        with step2:
            if is_weather:
                st.markdown('<div class="processing-step">[2/4] Weather query detected - using RAG workflow</div>', unsafe_allow_html=True)
            else:
                st.markdown('<div class="processing-step">[2/4] Data analysis query detected - using classification workflow</div>', unsafe_allow_html=True)
        
        time.sleep(0.5)
        
        # Step 3: Processing
        with step3:
            st.markdown('<div class="processing-step">[3/4] Processing with AI agent...</div>', unsafe_allow_html=True)
        
        # Actually process the query with timeout
        try:
            print(f"\n[Streamlit] Starting query processing: {user_query}")
            # Use asyncio.wait_for to add a timeout (300 seconds for LLM processing)
            result = await asyncio.wait_for(process_query(user_query), timeout=300.0)
            print(f"[Streamlit] Query processing completed, result length: {len(result) if result else 0}")

            with step4:
                st.markdown('<div class="processing-step">[4/4] Analysis complete!</div>', unsafe_allow_html=True)

            return result, None

        except asyncio.TimeoutError:
            print(f"[Streamlit] ERROR: Query timed out after 300s")
            with step4:
                st.markdown('<div class="processing-step">[ERROR] Processing timed out</div>', unsafe_allow_html=True)
            return None, "The query processing took too long. Please try a simpler question or check if the LLM service is available."
        except Exception as e:
            print(f"[Streamlit] ERROR: Exception during query processing: {type(e).__name__}: {e}")
            import traceback
            traceback.print_exc()
            with step4:
                st.markdown('<div class="processing-step">[ERROR] Error occurred during processing</div>', unsafe_allow_html=True)
            return None, str(e)

def main():
    """Main Streamlit application."""

    # Initialize session state for conversation memory
    if 'conversation_history' not in st.session_state:
        st.session_state.conversation_history = []
    if 'mentioned_entities' not in st.session_state:
        st.session_state.mentioned_entities = {'offices': set(), 'queries': []}

    # Header
    st.markdown('<h1 class="main-header">AI Office Assistant</h1>', unsafe_allow_html=True)
    st.markdown('<p style="text-align: center; color: #666;">Ask questions about office data, weather, and more using natural language</p>', unsafe_allow_html=True)

    # Sidebar with information
    with st.sidebar:
        st.header("System Status")
        
        # Check if MCP server is running
        try:
            import requests
            # Check if the MCP server process is responding on the port
            response = requests.get("http://127.0.0.1:8000/", timeout=2)
            if response.status_code in [200, 404, 405]:  # Server is responding
                st.success("MCP Server Connected")
            else:
                st.warning("MCP Server Status Unknown")
        except requests.exceptions.ConnectionError:
            st.error("MCP Server Offline")
            st.warning("Start the MCP server:\n```\npython mcp_server.py\n```")
        except:
            st.warning("MCP Server Status Unknown")
        
        st.header("Query Examples")
        st.markdown("""
        **Revenue Analysis:**
        - Which office has the highest revenue?
        - What's the average revenue?
        - Show me revenue statistics
        
        **Employee Analysis:**
        - Which office has the most employees?
        - How are employees distributed?
        
        **Office Profiles:**
        - Tell me about the Chicago office
        - What's the profile of New York?
        
        **Weather Queries:**
        - What's the weather at our Paris office?
        - Temperature in New York office
        """)
        
        st.header("Available Data")
        st.markdown("""
        - **Offices**: 10 locations
        - **Cities**: New York, Chicago, San Francisco, etc.
        - **Metrics**: Revenue, employees, opening year
        - **Weather**: Real-time data via API
        """)

        # Memory Dashboard
        st.header("Conversation Memory")

        # Memory metrics
        total_exchanges = len(st.session_state.conversation_history)
        st.metric("Total Exchanges", total_exchanges)

        if total_exchanges > 0:
            # Calculate approximate token usage (rough estimate)
            total_chars = sum(len(ex['query']) + len(ex.get('response', ''))
                            for ex in st.session_state.conversation_history)
            estimated_tokens = int(total_chars / 4)  # Rough estimate
            st.metric("Estimated Tokens", f"{estimated_tokens:,}")

            # Token usage warning
            max_tokens = 4000
            token_percentage = (estimated_tokens / max_tokens) * 100
            st.progress(min(token_percentage / 100, 1.0))

            if estimated_tokens > 3000:
                st.warning("Memory nearly full - consider clearing")

        # Entities tracked
        if st.session_state.mentioned_entities['offices']:
            st.subheader("Offices Discussed")
            for office in sorted(st.session_state.mentioned_entities['offices']):
                st.text(f"â€¢ {office}")

        # Memory controls
        col1, col2 = st.columns(2)
        with col1:
            if st.button("Clear Memory", use_container_width=True):
                st.session_state.conversation_history = []
                st.session_state.mentioned_entities = {'offices': set(), 'queries': []}
                st.success("Memory cleared!")
                st.rerun()

        with col2:
            if st.button("View History", use_container_width=True):
                st.session_state.show_history = not st.session_state.get('show_history', False)

        # Show conversation history if toggled
        if st.session_state.get('show_history', False) and total_exchanges > 0:
            with st.expander("Conversation History", expanded=True):
                for i, exchange in enumerate(reversed(st.session_state.conversation_history[-5:]), 1):
                    st.markdown(f"**{exchange['timestamp']}**")
                    st.text(f"Q: {exchange['query'][:60]}...")
                    if 'response' in exchange:
                        st.text(f"A: {exchange.get('response', '')[:60]}...")
                    st.divider()
    
    # Main query interface
    st.header("Ask Your Question")

    # Query input with Enter key support
    with st.form(key="query_form", clear_on_submit=True):
        user_query = st.text_input(
            "Enter your question:",
            placeholder="e.g., Which office has the highest revenue?",
            help="Ask about office data, weather, or any business metrics",
            key="user_query_input"
        )

        # Submit button
        submit_button = st.form_submit_button("Ask Assistant", type="primary", use_container_width=True)

    # Process query when form is submitted
    if submit_button and user_query:
        if user_query.strip():
            
            # Display the user query
            with st.container():
                st.markdown(f'<div class="query-box"><strong>Your Question:</strong> {user_query}</div>', unsafe_allow_html=True)
            
            # Process the query
            with st.spinner("Processing your question..."):
                try:
                    # Run async function in streamlit
                    result, error = asyncio.run(process_query_with_progress(user_query))
                    
                    if error:
                        st.error(f"Error: {error}")

                        # Show fallback message
                        if "timeout" in error.lower() or "long" in error.lower():
                            st.info("**Tip**: The AI model might be busy. Try using the simpler `app.py` version instead:\n```\nstreamlit run app.py\n```")
                        else:
                            st.info("**Alternative**: You can also use the embedded version that doesn't require external services:\n```\nstreamlit run app.py\n```")
                    elif result:
                        # Store in conversation memory
                        exchange = {
                            'query': user_query,
                            'response': result,
                            'timestamp': datetime.now().strftime("%H:%M:%S")
                        }
                        st.session_state.conversation_history.append(exchange)

                        # Extract and track entities (simple keyword matching)
                        office_names = ['Chicago', 'New York', 'San Francisco', 'Austin',
                                      'Atlanta', 'Boston', 'Denver', 'Seattle', 'Miami', 'Los Angeles']
                        for office in office_names:
                            if office.lower() in user_query.lower() or office.lower() in result.lower():
                                st.session_state.mentioned_entities['offices'].add(office)

                        # Display the result
                        st.markdown("### Assistant Response")
                        with st.container():
                            st.markdown(f'<div class="result-box">{result}</div>', unsafe_allow_html=True)

                        # Additional insights for data queries
                        if not any(word in user_query.lower() for word in ["weather", "temperature", "forecast"]):
                            st.markdown("### Query Insights")
                            col1, col2, col3 = st.columns(3)

                            with col1:
                                st.metric("Query Type", "Data Analysis")
                            with col2:
                                st.metric("Processing Time", "< 5s")
                            with col3:
                                st.metric("Confidence", "High")
                    
                except Exception as e:
                    st.error(f"Unexpected error: {e}")
                    st.info("Make sure the MCP server is running: `python mcp_server.py`")
        else:
            st.warning("Please enter a question to get started!")
    
    # Footer
    st.markdown("---")
    st.markdown(
        '<p style="text-align: center; color: #666; font-size: 0.8rem;">'
        'Powered by Query Classification Agent | Built with Streamlit'
        '</p>', 
        unsafe_allow_html=True
    )

if __name__ == "__main__":
    main()
