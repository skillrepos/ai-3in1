#!/usr/bin/env python3
"""
Lab 5: RAG-Enhanced Agentic Weather Agent (v2 — LLM Summaries)
────────────────────────────────────────────────────────────────────
A true agentic RAG workflow combining:
- Lab 2's agent pattern (TAO loop with LLM-driven tool selection)
- Lab 3's MCP server (weather and geocoding tools)
- Lab 4's vector database (ChromaDB with office PDF data)

The LLM controls the entire workflow, deciding which tools to call
and when to stop — just like the agents in Labs 2 and 3.

Tools Available to the Agent
----------------------------
1. search_offices(query) → text chunks from office vector DB (local RAG)
2. geocode_location(name) → lat/lon coordinates (MCP server)
3. get_weather(lat, lon) → current weather in Celsius (MCP server)
4. convert_c_to_f(c) → temperature in Fahrenheit (MCP server)

Example Agent Flow
------------------
  User: "Tell me about HQ"
  → Agent calls search_offices("HQ") → gets office info with city
  → Agent calls geocode_location("Austin") → gets coordinates
  → Agent calls get_weather(30.27, -97.74) → gets weather data
  → Agent calls convert_c_to_f(25.0) → gets Fahrenheit
  → Agent says DONE → LLM composes a friendly summary

Prerequisites
-------------
- ChromaDB populated: python tools/index_pdf.py (Lab 4)
- MCP server running: python mcp_server.py (Lab 3)
"""

# ────────────────────────── standard libs ───────────────────────────
import asyncio
import json
import re
import textwrap
from pathlib import Path

# ────────────────────────── third-party libs ────────────────────────
import chromadb
from chromadb.config import Settings, DEFAULT_TENANT, DEFAULT_DATABASE
from sentence_transformers import SentenceTransformer
from fastmcp import Client
from fastmcp.exceptions import ToolError
from langchain_ollama import ChatOllama

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 1.  Configuration                                               ║
# ╚══════════════════════════════════════════════════════════════════╝
CHROMA_PATH      = Path("./chroma_db")          # Where Lab 4 stored the vector DB
COLLECTION_NAME  = "codebase"                   # Collection name from Lab 4
EMBED_MODEL_NAME = "all-MiniLM-L6-v2"           # Same embedding model as Lab 4
MCP_ENDPOINT     = "http://127.0.0.1:8000/mcp/" # MCP server from Lab 3
TOP_K            = 3                            # Number of RAG results to retrieve

# Regex for parsing LLM responses (same pattern as Labs 2 and 3)
ACTION_RE = re.compile(r"Action:\s*(\w+)", re.IGNORECASE)
ARGS_RE   = re.compile(r"Args:\s*(\{.*?\})(?:\s|$)", re.S | re.IGNORECASE)

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 2.  RAG search tool (local — queries the ChromaDB from Lab 4)   ║
# ╚══════════════════════════════════════════════════════════════════╝
# Initialize the embedding model and database connection once at startup
embed_model = SentenceTransformer(EMBED_MODEL_NAME)

def open_collection() -> chromadb.Collection:
    """Open the ChromaDB collection populated in Lab 4."""
    client = chromadb.PersistentClient(
        path=str(CHROMA_PATH),
        settings=Settings(),
        tenant=DEFAULT_TENANT,
        database=DEFAULT_DATABASE,
    )
    return client.get_or_create_collection(COLLECTION_NAME)

coll = open_collection()

def search_offices(query: str) -> str:
    """
    Search the office vector database for relevant information.
    This is the 'Retrieval' part of RAG — semantic search over
    the office PDF data indexed in Lab 4.

    Returns the top matching text chunks as a string.
    """
    q_emb = embed_model.encode(query).tolist()
    res = coll.query(
        query_embeddings=[q_emb],
        n_results=TOP_K,
        include=["documents"],
    )
    docs = res["documents"][0] if res["documents"] else []
    if not docs:
        return "No matching office information found."
    return "\n---\n".join(docs)

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 3.  MCP result unwrapper                                        ║
# ╚══════════════════════════════════════════════════════════════════╝
def unwrap(obj):
    """Extract plain Python values from FastMCP result wrappers."""
    if hasattr(obj, "structured_content") and obj.structured_content:
        return unwrap(obj.structured_content)
    if hasattr(obj, "data") and obj.data:
        return unwrap(obj.data)
    if hasattr(obj, "text"):
        try:
            return json.loads(obj.text)
        except Exception:
            return obj.text
    if hasattr(obj, "value"):
        return obj.value
    if isinstance(obj, list) and len(obj) == 1:
        return unwrap(obj[0])
    if isinstance(obj, dict):
        numeric_vals = [v for v in obj.values() if isinstance(v, (int, float))]
        if len(numeric_vals) == 1:
            return numeric_vals[0]
    return obj

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 4.  System prompt — tells the LLM about all available tools     ║
# ╚══════════════════════════════════════════════════════════════════╝
SYSTEM = textwrap.dedent("""
You are an office information agent. You answer questions about company
offices by searching a database and looking up live weather data.

You have these tools:

search_offices(query: str)
    Searches the company office database for matching information.
    Returns: text chunks with office names, cities, and details.
    ALWAYS call this first to find relevant office information.

geocode_location(name: str)
    Converts a city/location name to coordinates.
    Returns: {"latitude": float, "longitude": float, "name": str}

get_weather(lat: float, lon: float)
    Gets current weather for given coordinates.
    Returns: {"temperature": float, "code": int, "conditions": str}
    Note: temperature is in Celsius.

convert_c_to_f(c: float)
    Converts a Celsius temperature to Fahrenheit.
    Returns: float

For each step, respond with EXACTLY these three lines:

Thought: <your reasoning about what to do next>
Action: <tool name: search_offices, geocode_location, get_weather, convert_c_to_f, or DONE>
Args: <valid JSON arguments for the tool>

When you have gathered all the information, respond with:
Thought: I have all the information needed
Action: DONE
Args: {}
Final: <a friendly 2-3 sentence summary including the office name and
city, weather conditions and temperature in Fahrenheit, and one
interesting fact about the city>

RULES:
1. ALWAYS start with search_offices to find office data
2. Extract the city from the search results, then geocode it
3. Get the weather, then convert the temperature
4. Office details and weather MUST come from tool results — do NOT invent them
5. You may add one interesting fact about the city from your own knowledge
6. Do NOT add extra text beyond the required format
""").strip()

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 5.  TAO agent loop — the LLM decides which tools to call        ║
# ╚══════════════════════════════════════════════════════════════════╝
async def run(prompt: str, max_steps: int = 10) -> None:
    """
    Run the agentic RAG loop where the LLM drives the workflow.

    The agent can call local tools (search_offices for RAG retrieval)
    and remote tools (geocode, weather, conversion via MCP server).
    """
    llm = ChatOllama(model="llama3.2", temperature=0.0)

    messages = [
        {"role": "system", "content": SYSTEM},
        {"role": "user",   "content": prompt},
    ]

    # Track gathered data as fallback for final display
    context = {
        "office_info": None,
        "city": None,
        "conditions": None,
        "temp_f": None,
    }

    print("\n" + "="*60)
    print("RAG Agent — Thought / Action / Observation")
    print("="*60 + "\n")

    async with Client(MCP_ENDPOINT) as mcp:
        for step in range(1, max_steps + 1):
            print(f"[Step {step}]")

            # Ask the LLM what to do next
            response = llm.invoke(messages).content.strip()
            print(response)

            # Parse the Action from the response
            action_match = ACTION_RE.search(response)
            if not action_match:
                print("\nError: Could not parse Action from response\n")
                break

            action = action_match.group(1).lower()

            # ── Check if the agent decided it is done ─────────────────
            if action == "done":
                print("\n" + "="*60)
                # Check for LLM-composed summary (v2 enhancement)
                if "Final:" in response:
                    final = response.split("Final:", 1)[1].strip()
                    print(f"\n{final}\n")
                else:
                    # Fallback to collected data
                    print("Agent completed!\n")
                    if context["office_info"]:
                        print(f"Office: {context['office_info']}")
                    if context["conditions"] and context["temp_f"] is not None:
                        print(f"Weather: {context['conditions']}, "
                              f"{context['temp_f']:.1f} °F")
                    print()
                return

            # ── Parse the Args ────────────────────────────────────────
            args_match = ARGS_RE.search(response)
            if not args_match:
                print("\nError: Could not parse Args from response\n")
                break

            try:
                args = json.loads(args_match.group(1))
            except json.JSONDecodeError as e:
                print(f"\nError: Invalid JSON: {e}\n")
                break

            # ── Call the tool — local (RAG) or remote (MCP) ──────────
            print(f"\n-> Calling: {action}({json.dumps(args)})")

            try:
                if action == "search_offices":
                    # Local tool: RAG vector search
                    result = search_offices(**args)
                    context["office_info"] = result.split("\n")[0][:200]
                else:
                    # MCP tools: geocode, weather, conversion
                    raw = await mcp.call_tool(action, args)
                    result = unwrap(raw)
            except ToolError as e:
                result = f"Error: {e}"
            except Exception as e:
                result = f"Error: {type(e).__name__}: {e}"

            # Store relevant context from MCP tool results
            if action == "geocode_location" and isinstance(result, dict):
                context["city"] = result.get("name")
            elif action == "get_weather" and isinstance(result, dict):
                context["conditions"] = result.get("conditions")
            elif action == "convert_c_to_f" and isinstance(result, (int, float)):
                context["temp_f"] = float(result)

            # Format the observation and show it
            if isinstance(result, dict):
                obs_text = json.dumps(result)
            else:
                obs_text = str(result)
            print(f"Observation: {obs_text}\n")

            # Feed the observation back to the LLM
            messages.append({"role": "assistant", "content": response})
            messages.append({"role": "user",
                             "content": f"Observation: {obs_text}"})

        print(f"\nReached maximum steps ({max_steps}).\n")

# ╔══════════════════════════════════════════════════════════════════╗
# ║ 6.  Interactive loop                                             ║
# ╚══════════════════════════════════════════════════════════════════╝
if __name__ == "__main__":
    print("="*60)
    print("RAG-Enhanced Office Weather Agent (v2 — LLM summaries)")
    print("="*60)
    print("\nAsk about any office (e.g. 'Tell me about HQ')")
    print("Type 'exit' to quit\n")

    while True:
        prompt = input("User: ").strip()
        if prompt.lower() == "exit":
            print("Goodbye!")
            break
        if prompt:
            asyncio.run(run(prompt))
            print()
